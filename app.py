import streamlit as st
import pandas as pd
import plotly.express as px
import gspread
from gspread_dataframe import get_as_dataframe
import matplotlib.pyplot as plt
import textwrap
import shutil
import os
from google.auth import default
from sentence_transformers import SentenceTransformer
import numpy as np

# ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
ORIGINAL_DIR = "data/original/"
WORKING_DIR = "data/working/"

def ensure_working_dir():
    if not os.path.exists(WORKING_DIR) or not os.listdir(WORKING_DIR):
        # ãƒ•ã‚©ãƒ«ãƒ€ãŒç„¡ã„ã‹ã€ä¸­èº«ãŒç©ºãªã‚‰originalã‹ã‚‰ã‚³ãƒ”ãƒ¼
        if os.path.exists(WORKING_DIR):
            shutil.rmtree(WORKING_DIR)  # ä¸­é€”åŠç«¯ãªç©ºãƒ•ã‚©ãƒ«ãƒ€ã¯å‰Šé™¤
        shutil.copytree(ORIGINAL_DIR, WORKING_DIR)

def save_working_to_original():
    if os.path.exists(WORKING_DIR):
        # working â†’ originalã«ä¸Šæ›¸ã
        for filename in os.listdir(WORKING_DIR):
            src = os.path.join(WORKING_DIR, filename)
            dst = os.path.join(ORIGINAL_DIR, filename)
            shutil.copy2(src, dst)

def step1_generate_args_csv():
    uploaded_file = st.file_uploader("ğŸ“ åˆæœŸã‚¢ã‚¤ãƒ‡ã‚¢ãƒªã‚¹ãƒˆï¼ˆCSVï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type="csv")

    if uploaded_file is not None:
        df_input = pd.read_csv(uploaded_file)
        df_input = df_input.rename(columns={"ç•ªå·": "comment-id", "ã‚¢ã‚¤ãƒ‡ã‚¢": "comment-body"})

        # arg-id ã¨ argument ã‚’ä½œæˆ
        df_args = df_input.copy()
        df_args["arg-id"] = df_args["comment-id"].apply(lambda cid: f"A{cid}_0")
        df_args["argument"] = df_args["comment-body"]
        df_args = df_args[["arg-id", "comment-id", "argument"]]

        # workingãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        working_dir = "data/working"
        os.makedirs(working_dir, exist_ok=True)
        df_args.to_csv(os.path.join(working_dir, "args.csv"), index=False)

        st.success("âœ… ã‚¢ã‚¤ãƒ‡ã‚¢ãƒªã‚¹ãƒˆã‚’å–ã‚Šè¾¼ã¿ã¾ã—ãŸï¼")
    else:
        st.info("ğŸ“‚ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

def step2_generate_embeddings():
    st.info("ğŸ“š ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ä¸­... å°‘ã€…ãŠå¾…ã¡ãã ã•ã„ã€‚")

    input_path = os.path.join(WORKING_DIR, "args.csv")
    output_path = os.path.join(WORKING_DIR, "embeddings.pkl")

    if not os.path.exists(input_path):
        st.error("âŒ args.csv ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ãƒ†ãƒƒãƒ—1ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    df = pd.read_csv(input_path)
    model = SentenceTransformer("cl-nagoya/sup-simcse-ja-large")

    texts = df["argument"].tolist()
    embeddings = model.encode(texts, show_progress_bar=True)

    df_embeds = pd.DataFrame({
        "arg-id": df["arg-id"],
        "embedding": embeddings.tolist()
    })

    df_embeds.to_pickle(output_path)
    st.success("âœ… embeddings.pkl ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")

def discard_working_changes():
    if os.path.exists(WORKING_DIR):
        shutil.rmtree(WORKING_DIR)
    shutil.copytree(ORIGINAL_DIR, WORKING_DIR)

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã‚¢ã‚¤ãƒ‡ã‚¢ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ—ãƒª",
    page_icon="ğŸ¯",
    layout="wide",  # â†ã“ã‚ŒãŒå¤§äº‹ï¼ï¼ˆæ¨ªå¹…ã‚’åºƒãä½¿ã†ï¼‰
    initial_sidebar_state="collapsed",
)

st.title("ğŸ¯ ã‚¢ã‚¤ãƒ‡ã‚¢ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ—ãƒª")


# ã‚¿ãƒ–ä½œæˆ
tab_summary, tab_analysis, tab_list, tab_settings = st.tabs(["ğŸ“Š ã‚µãƒãƒª", "ğŸ“– è§£èª¬", "ğŸ“‘ ä¸€è¦§", "âš™ï¸ è¨­å®šï¼ˆå®Ÿè£…ä¸­ï¼‰"])

# --- ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ args.csv ã‚’ç”Ÿæˆï¼ˆGroqãªã—ç‰ˆï¼‰ ---
@st.cache_data
def generate_args_from_spreadsheet(spreadsheet_id: str, worksheet_name: str = "Sheet1"):
    creds, _ = default()
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(spreadsheet_id)
    worksheet = sh.worksheet(worksheet_name)
    df_input = get_as_dataframe(worksheet, evaluate_formulas=True)

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ãƒªãƒãƒ¼ãƒ 
    df_input = df_input.rename(columns={"ç•ªå·": "comment-id", "ã‚¢ã‚¤ãƒ‡ã‚¢": "comment-body"})
    df_args = df_input.copy()
    df_args["arg-id"] = df_args["comment-id"].apply(lambda cid: f"A{cid}_0")
    df_args["argument"] = df_args["comment-body"]

    # å¿…è¦ãªåˆ—ã ã‘æ®‹ã™
    df_args = df_args[["arg-id", "comment-id", "argument"]]

    # ä¿å­˜
    df_args.to_csv(os.path.join(WORKING_DIR, "args.csv"), index=False)
    st.success("âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ args.csv ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼")

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
@st.cache_data
def load_data():
    ensure_working_dir()
    # CSVèª­ã¿è¾¼ã¿
    df_clusters = pd.read_csv(os.path.join(WORKING_DIR, "clusters.csv"))
    df_takeaways = pd.read_csv(os.path.join(WORKING_DIR, "takeaways.csv"))
    df_labels = pd.read_csv(os.path.join(WORKING_DIR, "labels.csv"))
    df_ideas = pd.read_csv(os.path.join(WORKING_DIR, "clustered_ideas.csv"))
    df_args = pd.read_csv(os.path.join(WORKING_DIR, "args.csv"))

    def matplotlib_color_to_hex(color_tuple):
        r, g, b, a = color_tuple
        return '#{:02x}{:02x}{:02x}'.format(int(r * 255), int(g * 255), int(b * 255))

    # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’ã“ã“ã§å‹•çš„ç”Ÿæˆ
    cluster_ids = sorted(df_clusters["cluster-id"].unique())
    cmap = plt.get_cmap("tab10")
    cluster_color_map = {str(cid): matplotlib_color_to_hex(cmap(i % 10)) for i, cid in enumerate(cluster_ids)}

    return df_clusters, df_takeaways, df_labels, df_ideas, df_args, cluster_color_map

# ã“ã“ã‚‚æ•°åˆã‚ã›ã‚‹ï¼ï¼ˆ6å€‹è¿”ã£ã¦ãã‚‹ï¼‰
df_clusters, df_takeaways, df_labels, df_ideas, df_args, cluster_color_map = load_data()

# --- ãƒ‡ãƒ¼ã‚¿æ•´å½¢ ---
df_clusters = df_clusters.merge(df_labels, on="cluster-id", how="left")
df_clusters = df_clusters.merge(df_takeaways, on="cluster-id", how="left")
df_clusters = df_clusters.merge(df_args[["arg-id", "argument"]], on="arg-id", how="left")

# --- ã‚µãƒãƒªã‚¿ãƒ– ---
with tab_summary:
    st.header("ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ")
    # æœ€å¤§è¡¨ç¤ºä»¶æ•°
    MAX_POINTS = 1000  # æœ€å¤§è¡¨ç¤ºä»¶æ•°

    # å¿…è¦ãªã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if len(df_clusters) > MAX_POINTS:
        df_plot = df_clusters.sample(MAX_POINTS, random_state=42)
    else:
        df_plot = df_clusters

    # ã‚°ãƒ©ãƒ•ä½œæˆ
    fig = px.scatter(
        df_plot,
        x="x",
        y="y",
        color=df_plot["cluster-id"].astype(str),  # ã‚¯ãƒ©ã‚¹ã‚¿IDã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        color_discrete_map=cluster_color_map,  # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚‚æ–‡å­—åˆ—ã‚­ãƒ¼
        hover_data=["argument", "label"],
        title="ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœï¼ˆãƒã‚¦ã‚¹ã‚ªãƒ¼ãƒãƒ¼ã§è©³ç´°è¡¨ç¤ºï¼‰"
    )

    # âœ¨ã“ã“ãŒè¿½åŠ ãƒ»ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ
    fig.update_layout(
        autosize=True,
        width=800,  # åˆæœŸã‚µã‚¤ã‚º 4:3
        height=600,
        margin=dict(l=0, r=0, t=30, b=0),
        yaxis=dict(scaleanchor="x", scaleratio=1),  # ç¸¦æ¨ªæ¯”å›ºå®š
    )

    # Streamlitã«æç”»
    st.plotly_chart(fig, use_container_width=True)

    # --- ã‚¹ãƒ†ãƒƒãƒ—6ã®æ¦‚è¦è¡¨ç¤º ---
    try:
        with open(os.path.join(WORKING_DIR, "overview.txt"), "r", encoding="utf-8") as f:
            overview_text = f.read()
            st.markdown(overview_text)
    except FileNotFoundError:
        st.warning("âš ï¸ overview.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


# --- è§£èª¬ã‚¿ãƒ– ---
with tab_analysis:
    st.header("ğŸ“– ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥è©³ç´°")

    for cluster_id in sorted(df_clusters["cluster-id"].unique()):
        cluster_data = df_clusters[df_clusters["cluster-id"] == cluster_id]
        label = cluster_data["label"].values[0]
        takeaway = cluster_data["takeaways"].values[0]

        count = len(cluster_data)
        percent = round((count / len(df_clusters)) * 100, 1)
        st.subheader(f"ğŸŸ¦ ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}ã€Œ{label}ã€ï¼ˆ{count}ä»¶ / {percent}%ï¼‰")

        # â–¶ï¸ ãƒã‚¦ã‚¹ã‚ªãƒ¼ãƒãƒ¼ã§ãã‚‹å¯è¦–åŒ–ã‚’è¡¨ç¤º
        plot_df = df_clusters.copy()
        plot_df["highlight_color"] = plot_df["cluster-id"].apply(
            lambda cid: cluster_color_map[str(cid)] if cid == cluster_id else "#d3d3d3"
        )
        plot_df["hover_text"] = "ã‚¯ãƒ©ã‚¹ã‚¿ " + plot_df["cluster-id"].astype(str) + "<br>" + plot_df["argument"]

        fig = px.scatter(
            plot_df,
            x="x",
            y="y",
            color="highlight_color",
            color_discrete_map="identity",
            hover_name="hover_text",
            title=f"Cluster {cluster_id}: {label}",
            width=800,   # â†ã“ã“ 800ã«å¤‰æ›´ï¼ï¼ˆ4:3ï¼‰
            height=600,  # â†ã“ã“ 600ã«å¤‰æ›´ï¼ï¼ˆ4:3ï¼‰
        )
        fig.update_traces(marker=dict(size=8, opacity=0.9), selector=dict(mode='markers'))
        fig.update_layout(
            showlegend=False,
            margin=dict(l=0, r=0, t=30, b=0),
            yaxis=dict(scaleanchor="x", scaleratio=1)  # ç¸¦æ¨ªæ¯”ã‚’å›ºå®šï¼
        )
        st.plotly_chart(fig, use_container_width=True)

        # â–¶ï¸ è¦ç´„ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¤º
        st.markdown(f"**è¦ç´„:** {takeaway}")
        st.divider()

# --- ä¸€è¦§ã‚¿ãƒ– ---
with tab_list:
    st.header("ğŸ“‘ ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥ãƒã‚°ãƒªã‚¹ãƒˆä¸€è¦§")

    # ğŸ”¥ ã‚¯ãƒ©ã‚¹ã‚¿ID or ãƒ©ãƒ™ãƒ«ã§ãƒ•ã‚£ãƒ«ã‚¿ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    cluster_options = df_ideas["cluster-id"].unique()
    selected_cluster = st.selectbox("è¡¨ç¤ºã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠ", options=["å…¨ã¦"] + sorted(cluster_options.tolist()))

    # ğŸ”¥ é¸æŠã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’çµã‚‹
    if selected_cluster == "å…¨ã¦":
        df_display = df_ideas.copy()
    else:
        df_display = df_ideas[df_ideas["cluster-id"] == selected_cluster]

    # ğŸ”¥ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¡¨ç¤ºã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
    st.dataframe(
        df_display.style.set_properties(**{
            'text-align': 'left',
            'white-space': 'pre-wrap',   # é•·æ–‡ã§ã‚‚æŠ˜ã‚Šè¿”ã™
        }),
        hide_index=True,
        use_container_width=True,
        column_config={
            "label": st.column_config.TextColumn(width="large"),    # ãƒ©ãƒ™ãƒ«åˆ—ã‚’åºƒã’ã‚‹
            "argument": st.column_config.TextColumn(width="large"), # ãƒã‚°ãƒªã‚¹ãƒˆåˆ—ã‚’åºƒã’ã‚‹
        }
    )

    st.markdown("---")
    st.subheader("ğŸ” é¡ä¼¼ãƒã‚°ãƒªã‚¹ãƒˆæ¤œç´¢ï¼†LLMå›ç­”")

    input_text = st.text_input("ğŸ’¬ ã‚ãªãŸã®ãƒã‚°ãƒªã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    top_k = st.slider("ğŸ”¢ ä½•ä»¶ã¾ã§å€™è£œã‚’å‡ºã™ã‹ï¼Ÿ", min_value=3, max_value=20, value=5)

    if st.button("ğŸš€ é¡ä¼¼ãƒã‚°ãƒªã‚¹ãƒˆã‚’æ¤œç´¢"):
        if input_text.strip() == "":
            st.warning("âš ï¸ å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("ğŸ” é¡ä¼¼ãƒã‚°ãƒªã‚¹ãƒˆã‚’æ¤œç´¢ä¸­..."):
                from sklearn.metrics.pairwise import cosine_similarity
                from sentence_transformers import SentenceTransformer
                from openai import OpenAI

                # ãƒ¢ãƒ‡ãƒ«é¸æŠUI
                model_choice = st.selectbox("ğŸ” ä½¿ç”¨ã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", [
                    "OpenAI API (text-embedding-3-small)",
                    "SentenceTransformer (multilingual-e5-small)",
                    "SentenceTransformer (sup-simcse-ja-large)",
                ])

                if model_choice == "OpenAI API (text-embedding-3-small)":
                    client = OpenAI(api_key=st.secrets["openai_api_key"])
                    response = client.embeddings.create(
                        model="text-embedding-3-small",
                        input=[input_text]
                    )
                    # OpenAI returns a list of embeddings
                    input_vec = np.array([record.embedding for record in response.data])
                elif model_choice == "SentenceTransformer (multilingual-e5-small)":
                    model = SentenceTransformer("intfloat/multilingual-e5-small")
                    input_vec = model.encode([input_text])
                elif model_choice == "SentenceTransformer (sup-simcse-ja-large)":
                    model = SentenceTransformer("cl-nagoya/sup-simcse-ja-large")
                    input_vec = model.encode([input_text])
                else:
                    st.error("âŒ ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¨ãƒ©ãƒ¼")
                    st.stop()

                # æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«èª­ã¿è¾¼ã¿
                df_embed = pd.read_pickle(os.path.join(WORKING_DIR, "embeddings.pkl"))
                embeddings = np.vstack(df_embed["embedding"].values)

                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
                similarities = cosine_similarity(input_vec, embeddings)[0]

                # é¡ä¼¼åº¦Top Kã‚’å–å¾—
                top_indices = similarities.argsort()[-top_k:][::-1]

                similar_args = df_args.iloc[top_indices].copy()
                similar_args["similarity"] = similarities[top_indices]

                # ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã‚’ãƒãƒ¼ã‚¸
                similar_args = similar_args.merge(
                    df_clusters[["arg-id", "cluster-id", "label"]],
                    on="arg-id",
                    how="left"
                )

                # è¡¨ç¤ºç”¨æ•´å½¢
                df_show = similar_args[["similarity", "cluster-id", "label", "argument"]]
                df_show = df_show.rename(columns={"similarity": "é¡ä¼¼åº¦", "cluster-id": "ã‚¯ãƒ©ã‚¹ã‚¿ID", "label": "ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«", "argument": "ãƒã‚°ãƒªã‚¹ãƒˆ"})
                df_show = df_show.sort_values(by="é¡ä¼¼åº¦", ascending=False)

                st.success(f"âœ… é¡ä¼¼åº¦ãƒˆãƒƒãƒ—{top_k}ä»¶ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
                st.dataframe(
                    df_show.style.set_properties(**{
                        'text-align': 'left',
                        'white-space': 'pre-wrap',
                    }),
                    hide_index=True,
                    use_container_width=True,
                    column_config={
                        "ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«": st.column_config.TextColumn(width="large"),
                        "ãƒã‚°ãƒªã‚¹ãƒˆ": st.column_config.TextColumn(width="large"),
                    }
                )

            if st.button("ğŸ¤– LLMã«æœ€ã‚‚ä¼¼ãŸãƒã‚°ãƒªã‚¹ãƒˆã‚’é¸ã°ã›ã‚‹"):
                with st.spinner("ğŸ¤– LLMãŒè€ƒãˆä¸­..."):
                    import requests
                    url = "https://api.groq.com/openai/v1/chat/completions"
                    headers = {
                        "Authorization": f"Bearer {st.secrets['groq_api_key']}",
                        "Content-Type": "application/json"
                    }

                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿ç«‹ã¦
                    prompt = "ä»¥ä¸‹ã®ãƒã‚°ãƒªã‚¹ãƒˆã®ä¸­ã§ã€ã‚ãªãŸã®ãƒã‚°ãƒªã‚¹ãƒˆã€Œ{}ã€ã«æœ€ã‚‚æ„å‘³ãŒè¿‘ã„ã‚‚ã®ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚\n\n".format(input_text)
                    for idx, row in similar_args.iterrows():
                        prompt += f"- ({row['comment-id']}) {row['argument']}\n"

                    prompt += "\næœ€ã‚‚è¿‘ã„ã‚‚ã®ã® (ç•ªå·) ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"

                    system_message = {
                        "role": "system",
                        "content": "ã‚ãªãŸã¯ä¸ãˆã‚‰ã‚ŒãŸãƒªã‚¹ãƒˆã‹ã‚‰ã€æœ€ã‚‚æ„å‘³ãŒè¿‘ã„ã‚‚ã®ã‚’é¸ã¶AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                    }

                    response = requests.post(url, headers=headers, json={
                        "model": "llama3-70b-8192",
                        "messages": [
                            system_message,
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.0
                    })

                    if response.status_code == 200:
                        res_json = response.json()
                        answer = res_json["choices"][0]["message"]["content"].strip()
                        st.success(f"ğŸ¤– LLMã®å›ç­”: {answer}")
                    else:
                        st.error("âŒ LLMãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# --- è¨­å®šã‚¿ãƒ– ---
with tab_settings:
    st.header("âš™ï¸ è¨­å®šãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼ˆå®Ÿè£…ä¸­ï¼‰")
    
    # ğŸ”¥ å…¬é–‹ç‰ˆã§ã¯è¨­å®šã‚’å°å°
    if "developer_mode" in st.secrets and st.secrets["developer_mode"]:
        # é–‹ç™ºè€…ã ã‘ãŒè¨­å®šæ©Ÿèƒ½ã‚’è¦‹ã‚Œã‚‹ï¼ˆst.secrets["developer_mode"] = true ãªã‚‰è¡¨ç¤ºï¼‰

        with st.expander("ğŸ”„ ãƒ‡ãƒ¼ã‚¿å†å–å¾—ã¨å†å®Ÿè¡Œ"):
            st.write("ãƒ‡ãƒ¼ã‚¿å†å–å¾—ã‚„å†ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†å ´åˆã¯ã€ä»¥ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚")

            col1, col2, col3 = st.columns(3)

            with col1:
                if st.button("ğŸš€ ãƒ‡ãƒ¼ã‚¿å†å–å¾—ï¼‹ä¸€æ‹¬å®Ÿè¡Œ"):
                    st.session_state["action"] = "reload_all"

            with col2:
                if st.button("ğŸŒ€ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã ã‘å†è¨­å®š"):
                    st.session_state["action"] = "recluster_only"

            with col3:
                if st.button("ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´ â†’ ãƒ©ãƒ™ãƒ«å†ç”Ÿæˆ"):
                    st.session_state["action"] = "relabel_only"

        st.subheader("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š")
        new_prompt = st.text_area("ğŸ“ ãƒ©ãƒ™ãƒªãƒ³ã‚°ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", value="ï¼ˆã“ã“ã«åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥ã‚Œã‚‹ï¼‰", height=200)
        if st.button("ğŸ’¾ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜"):
            st.success("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ï¼ˆä»®ä¿å­˜ï¼‰")

        st.subheader("ä¿å­˜ãƒ»ç ´æ£„ãƒ¡ãƒ‹ãƒ¥ãƒ¼")

        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ’¾ ã“ã®ãƒ‡ãƒ¼ã‚¿ã§ä¿å­˜ï¼ˆæ¬¡å›èµ·å‹•æ™‚ã«åæ˜ ï¼‰", type="primary"):
                save_working_to_original()
                st.success("âœ… ä¿å­˜ã—ã¾ã—ãŸï¼ã‚¢ãƒ—ãƒªã‚’å†èµ·å‹•ã™ã‚‹ã¨åæ˜ ã•ã‚Œã¾ã™ã€‚")
                st.experimental_rerun()

        with col2:
            if st.button("ğŸ—‘ï¸ å¤‰æ›´ã‚’ç ´æ£„ï¼ˆèµ·å‹•æ™‚çŠ¶æ…‹ã«æˆ»ã™ï¼‰"):
                discard_working_changes()
                st.warning("âš ï¸ å¤‰æ›´ã‚’ç ´æ£„ã—ã¾ã—ãŸã€‚ã‚¢ãƒ—ãƒªã‚’å†èµ·å‹•ã—ã¾ã™ã€‚")
                st.experimental_rerun()

    else:
        st.info("ğŸš§ ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯é–‹ç™ºä¸­ã§ã™ã€‚è¿‘æ—¥å…¬é–‹äºˆå®šã€‚")

# æ–°ã—ã„é–¢æ•°ã‚’è¿½åŠ 
def step3_clustering(n_clusters: int):
    import numpy as np
    from sklearn.cluster import SpectralClustering
    from umap import UMAP
    from hdbscan import HDBSCAN
    from bertopic import BERTopic
    from sklearn.feature_extraction.text import CountVectorizer
    import nltk

    # nltkã®stopwordsãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ï¼‰
    nltk.download('stopwords')

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df_args = pd.read_csv(os.path.join(WORKING_DIR, "args.csv"))
    df_embed = pd.read_pickle(os.path.join(WORKING_DIR, "embeddings.pkl"))

    # ãƒãƒ¼ã‚¸
    df = pd.merge(df_args, df_embed, on="arg-id")

    docs = df["argument"].tolist()
    embeddings = np.vstack(df["embedding"].values)

    # æ¬¡å…ƒåœ§ç¸®ï¼ˆUMAPï¼‰
    umap_model = UMAP(random_state=42, n_components=2)
    umap_embeds = umap_model.fit_transform(embeddings)

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆHDBSCANï¼‰
    hdbscan_model = HDBSCAN(min_cluster_size=2)

    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆæ—¥æœ¬èªå¯¾å¿œãªã®ã§stopwordsãªã—ï¼‰
    vectorizer_model = CountVectorizer(stop_words=None)

    # BERTopic
    topic_model = BERTopic(
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        vectorizer_model=vectorizer_model,
        language="japanese",
        verbose=True,
    )

    topics, probs = topic_model.fit_transform(docs, embeddings)

    # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    spectral_model = SpectralClustering(
        n_clusters=n_clusters,
        affinity="nearest_neighbors",
        n_neighbors=min(10, len(embeddings) - 1),
        random_state=42
    )
    cluster_labels = spectral_model.fit_predict(umap_embeds)

    # æƒ…å ±ã¾ã¨ã‚
    doc_info = topic_model.get_document_info(docs)
    doc_info["arg-id"] = df["arg-id"]
    doc_info["comment-id"] = df["comment-id"]
    doc_info["x"] = umap_embeds[:, 0]
    doc_info["y"] = umap_embeds[:, 1]
    doc_info["cluster-id"] = cluster_labels

    # ä¿å­˜
    doc_info[["arg-id", "comment-id", "x", "y", "cluster-id"]].to_csv(
        os.path.join(WORKING_DIR, "clusters.csv"), index=False
    )
#    print("âœ… clusters.csv ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")
    print("âœ… clusters.csv ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")

# --- New functions added after step3_clustering ---
import requests
import time

# ã‚¹ãƒ†ãƒƒãƒ—4ç”¨ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦
def build_prompt(question, inside_list, outside_list):
    example = """
ã€ä¾‹ï¼šBrexitã«é–¢ã™ã‚‹è­°è«–ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒªãƒ³ã‚°ã€‘

ã‚³ãƒ³ã‚µãƒ«ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è³ªå•ï¼šã€Œè‹±å›½ã®EUé›¢è„±æ±ºå®šã®å½±éŸ¿ã¯ä½•ã ã¨æ€ã„ã¾ã™ã‹ï¼Ÿã€

é–¢å¿ƒã®ã‚ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä»¥å¤–ã®è«–ç‚¹ã®ä¾‹ï¼š
 * ã‚¨ãƒ©ã‚¹ãƒ ã‚¹ãƒ»ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ã®é™¤å¤–ã«ã‚ˆã‚Šã€æ•™è‚²ãƒ»æ–‡åŒ–äº¤æµã®æ©Ÿä¼šãŒåˆ¶é™ã•ã‚ŒãŸã€‚
 * ç’°å¢ƒåŸºæº–ã«ãŠã‘ã‚‹å”åŠ›ãŒæ¸›å°‘ã—ã€æ°—å€™å¤‰å‹•ã¨é—˜ã†åŠªåŠ›ãŒå¦¨ã’ã‚‰ã‚ŒãŸã€‚
 * å®¶æ—ã®å±…ä½æ¨©ã‚„å¸‚æ°‘æ¨©ã®ç”³è«‹ãŒè¤‡é›‘ã«ãªã£ãŸã€‚
 * åŒ»ç™‚å”å®šã®ä¸­æ–­ã€æ‚£è€…ã¸ã®å½±éŸ¿ã€‚
 * EUã®æ–‡åŒ–åŠ©æˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ã®é™¤å¤–ã«ã‚ˆã‚Šã€å‰µé€ çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆ¶é™ã€‚

ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…éƒ¨ã§ã®è­°è«–ã®ä¾‹ï¼š
 * ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ã®æ··ä¹±ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆå¢—ã¨ç´æœŸé…å»¶ã€‚
 * æŠ•è³‡ä¸å®‰ãƒ»é€€è·é‡‘ã®ä¾¡å€¤å¤‰å‹•ã€‚
 * è¼¸å‡ºæ¥­è€…ã®é–¢ç¨å¯¾å¿œã¨åˆ©ç›Šæ¸›å°‘ã€‚
 * ãƒãƒ³ãƒ‰å®‰ã¨ç”Ÿæ´»è²»ã®ä¸Šæ˜‡ã€‚

ã“ã®ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚«ãƒ†ã‚´ãƒªãƒ©ãƒ™ãƒ«ä¾‹ï¼šçµŒæ¸ˆçš„å½±éŸ¿
"""

    inside_text = '\n * ' + '\n * '.join(inside_list)
    outside_text = '\n * ' + '\n * '.join(outside_list)

    return f"""
{example}

----

ã€æœ¬é¡Œã€‘

ã‚³ãƒ³ã‚µãƒ«ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è³ªå•ï¼šã€Œ{question}ã€

é–¢å¿ƒã®ã‚ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä»¥å¤–ã®è«–ç‚¹ã®ä¾‹ï¼š
{outside_text}

ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…éƒ¨ã§ã®è­°è«–ã®ä¾‹ï¼š
{inside_text}

ã“ã®ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚«ãƒ†ã‚´ãƒªãƒ©ãƒ™ãƒ«ã‚’æ—¥æœ¬èªã§1ã¤ã€ä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ã¨æ„å‘³ãŒè¢«ã‚‰ãªã„ã‚ˆã†ãªã‚‚ã®ã‚’ç°¡æ½”ã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

# ã‚¹ãƒ†ãƒƒãƒ—4ç”¨ï¼šGroq APIå‘¼ã³å‡ºã—
def call_groq(prompt, max_retries=3):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {st.secrets['groq_api_key']}",
        "Content-Type": "application/json"
    }

    system_message = {
        "role": "system",
        "content": (
            "ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿å†…ãƒ»å¤–ã®æ„è¦‹ã‚’æ¯”è¼ƒã—ã€ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æœ€ã‚‚ç‰¹å¾´ã¥ã‘ã‚‹"
            "ã‚«ãƒ†ã‚´ãƒªãƒ©ãƒ™ãƒ«ã‚’æ—¥æœ¬èªã®åè©å¥ã§1ã¤è¿”ã™ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
            "å‡ºåŠ›ã¯ãƒ©ãƒ™ãƒ«ã®ã¿ã€‚10èªä»¥å†…ã§ã€‚\n"
            "ãƒ©ãƒ™ãƒ«ã¯æŠ½è±¡èªï¼ˆä¾‹ï¼šä¸ä¾¿ã•ã€å•é¡Œï¼‰ã ã‘ã§ãªãã€"
            "ä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨æ¯”ã¹å…·ä½“çš„ã«åˆ†é¡ã®å·®ãŒã‚ã‹ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šå­¦æ ¡ç”Ÿæ´»ã®ä¸ä¾¿ã€ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä¸Šã®ä¸æº€ãªã©ï¼‰ã€‚\n"
            "è§£é‡ˆãŒåºƒã™ãã‚‹è¡¨ç¾ï¼ˆæ—¥å¸¸ã‚„æ—¥å¸¸ç”Ÿæ´»ï¼‰ã‚„æ„å‘³ãŒåŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ã¨ãªã‚‰ãªã„ã‚ˆã†ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚"
        )
    }

    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json={
                "model": "llama3-70b-8192",
                "messages": [
                    system_message,
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.0
            })

            if response.status_code == 200:
                res_json = response.json()
                return res_json["choices"][0]["message"]["content"].strip()
            else:
                print(f"âš  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}, å†è©¦è¡Œ")
                time.sleep(2)
        except Exception as e:
            print("âŒ Error:", e)
            time.sleep(2)
    return "ãƒ©ãƒ™ãƒ«æœªå–å¾—"

# ã‚¹ãƒ†ãƒƒãƒ—4ï¼šãƒ©ãƒ™ãƒ«ç”Ÿæˆ
def step4_generate_labels(sample_size=10):
    st.info("ğŸ·ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ã«ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã¦ã„ã¾ã™... å°‘ã€…ãŠå¾…ã¡ãã ã•ã„ã€‚")

    input_args = os.path.join(WORKING_DIR, "args.csv")
    input_clusters = os.path.join(WORKING_DIR, "clusters.csv")
    output_labels = os.path.join(WORKING_DIR, "labels.csv")

    if not os.path.exists(input_args) or not os.path.exists(input_clusters):
        st.error("âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return

    df_args = pd.read_csv(input_args)
    df_clusters = pd.read_csv(input_clusters)

    question = "Discordã«æŠ•ç¨¿ã•ã‚ŒãŸã€Œç”Ÿæ´»ä¸Šã®å›°ã‚Šã”ã¨ãƒ»ä¸æº€ãƒ»æ”¹å–„ææ¡ˆã€ã‚’ã€ç¨®é¡ã‚„çŠ¶æ³ã«å¿œã˜ã¦åˆ†é¡ã—ã¦ãã ã•ã„ã€‚"
    cluster_ids = sorted(df_clusters["cluster-id"].unique())
    labels = []

    for cluster_id in cluster_ids:
        inside_ids = df_clusters[df_clusters["cluster-id"] == cluster_id]["arg-id"]
        outside_ids = df_clusters[df_clusters["cluster-id"] != cluster_id]["arg-id"]

        inside_sample = df_args[df_args["arg-id"].isin(inside_ids)].sample(min(sample_size, len(inside_ids)))
        outside_sample = df_args[df_args["arg-id"].isin(outside_ids)].sample(min(sample_size, len(outside_ids)))

        prompt = build_prompt(
            question,
            inside_sample["argument"].tolist(),
            outside_sample["argument"].tolist()
        )

        label = call_groq(prompt)
        labels.append({"cluster-id": cluster_id, "label": label})
        print(f"âœ… cluster {cluster_id}: {label}")
        time.sleep(1.0)

    df_labels = pd.DataFrame(labels)
    df_labels.to_csv(output_labels, index=False)
    st.success("âœ… labels.csv ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.caption("Â© 2025 Web3Gairon Broad Listening Project")